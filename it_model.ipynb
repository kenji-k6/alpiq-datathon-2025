{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9946ab2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:  (22632, 3399)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from data import create_dataset_splits\n",
    "\n",
    "train_df, forecast_df = create_dataset_splits(\n",
    "  data_dir=r\"datasets2025\",\n",
    "  processed_data_dir=r\"processed\",\n",
    "  country_code=\"IT\"\n",
    ")\n",
    "\n",
    "print(\"Shape: \", train_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e79fc908",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cols = [col[:20] == 'VALUEMWHMETERINGDATA' for col in train_df.columns]\n",
    "\n",
    "X = train_df.iloc[:-1,:].copy()\n",
    "Y = train_df.loc[:,y_cols].shift(-1).iloc[:-1,:].copy()\n",
    "assert X.isna().sum().sum() == 0 and Y.isna().sum().sum() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ece6757",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.index = pd.to_datetime(X.index)\n",
    "\n",
    "month = X.index.month\n",
    "weekday = X.index.weekday\n",
    "hour = X.index.hour\n",
    "\n",
    "X.loc[:,'month_sin'] = np.sin(2 * np.pi * month / 12)\n",
    "X.loc[:,'month_cos'] = np.cos(2 * np.pi * month / 12)\n",
    "X.loc[:,'day_sin'] = np.sin(2 * np.pi * weekday / 7)\n",
    "X.loc[:,'day_cos'] = np.cos(2 * np.pi * weekday / 7)\n",
    "X.loc[:,'hour_sin'] = np.sin(2 * np.pi * hour / 24)\n",
    "X.loc[:,'hour_cos'] = np.cos(2 * np.pi * hour / 24)\n",
    "\n",
    "cols_cat = [\n",
    "    'month_sin','month_cos',\n",
    "    'day_sin','day_cos',\n",
    "    'hour_sin','hour_cos'\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b87f933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed for reproducibility\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def plot_loss(train_losses, val_losses):\n",
    "    # Set up subplots\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(20, 6)) # 1 row, 3 columns\n",
    "    # First plot: Loss evolution (val loss alone)\n",
    "    axes[0].set_title('Loss evolution (val loss alone)')\n",
    "    axes[0].plot(train_losses, label='validation', marker='o')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_xlabel('Epochs')\n",
    "    axes[0].legend()\n",
    "    axes[1].set_title('Loss evolution (val loss alone)')\n",
    "    axes[1].plot(val_losses, label='validation', marker='o')\n",
    "    axes[1].set_ylabel('Loss')\n",
    "    axes[1].set_xlabel('Epochs')\n",
    "    axes[1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5b7c2db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xlstm.xlstm_large import xLSTMLarge as xLSTMModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8acd63d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "xLSTMLarge.__init__() takes 2 positional arguments but 6 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     35\u001b[39m max_grad_norm = \u001b[32m5.0\u001b[39m  \u001b[38;5;66;03m# Maximum gradient norm for clipping\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# dropout = 0.5: BAD\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# batch_size = 300: BAD\u001b[39;00m\n\u001b[32m     39\u001b[39m \n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# Initialize the model, loss function, and optimizer\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m model = \u001b[43mxLSTMLarge\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m criterion = nn.L1Loss()\n\u001b[32m     43\u001b[39m optimizer = torch.optim.Adam(model.parameters(), lr=\u001b[32m0.001\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: xLSTMLarge.__init__() takes 2 positional arguments but 6 were given"
     ]
    }
   ],
   "source": [
    "seed = 1  # You can choose any integer value\n",
    "set_seed(seed)\n",
    "\n",
    "# Split the data into training (first 80%) and validation (last 20%)\n",
    "\n",
    "# Standardize the data\n",
    "scaler_X = StandardScaler()\n",
    "scaler_Y = StandardScaler()\n",
    "\n",
    "# Fit the scalers on the training data and transform both training and validation data\n",
    "X_train = scaler_X.fit_transform(X.drop(cols_cat, axis=1))\n",
    "Y_train = scaler_Y.fit_transform(Y)\n",
    "\n",
    "X_train_cat = X[cols_cat]\n",
    "\n",
    "X_train = np.concatenate([X_train, X_train_cat], axis=1)\n",
    "\n",
    "# Convert the data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "Y_train_tensor = torch.tensor(Y_train, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoader for batching\n",
    "batch_size = 200\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, Y_train_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Model parameters\n",
    "input_dim = X_train.shape[1]  # Number of features (e.g., energy consumption + naive forecasts)\n",
    "hidden_dim = 168  # Increased hidden dimension for better capacity # (24,168)\n",
    "output_dim = Y_train.shape[1]  # Number of firms (or target variables)\n",
    "num_layers = 3  # Deeper LSTM\n",
    "dropout = 0.3  # Dropout to prevent overfitting\n",
    "max_grad_norm = 5.0  # Maximum gradient norm for clipping\n",
    "\n",
    "# dropout = 0.5: BAD\n",
    "# batch_size = 300: BAD\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = xLSTMModel(input_dim, hidden_dim, output_dim, num_layers, dropout)\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Early stopping parameters\n",
    "num_epochs = 500\n",
    "best_val_loss = float('inf')\n",
    "patience = 25\n",
    "epochs_no_improve = 0\n",
    "best_model_path = \"best_lstm_model.pth\"\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Training loop with early stopping\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for X_batch, Y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch.unsqueeze(1))  # Add sequence dimension\n",
    "        loss = criterion(outputs, Y_batch)\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    # Validation phase\n",
    "    # model.eval()\n",
    "    # val_loss = 0\n",
    "    # with torch.no_grad():\n",
    "    #     for X_batch, Y_batch in val_loader:\n",
    "    #         outputs = model(X_batch.unsqueeze(1))  # Add sequence dimension\n",
    "    #         loss = criterion(outputs, Y_batch)\n",
    "    #         val_loss += loss.item()\n",
    "\n",
    "    # val_loss /= len(val_loader)\n",
    "    # val_losses.append(val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "    # Early stopping logic\n",
    "    # if val_loss < best_val_loss:\n",
    "    #     best_val_loss = val_loss\n",
    "    #     epochs_no_improve = 0\n",
    "    #     torch.save(model.state_dict(), best_model_path)  # Save the best model\n",
    "    # else:\n",
    "    #     epochs_no_improve += 1\n",
    "    #     if epochs_no_improve >= patience:\n",
    "    #         print(\"Early stopping triggered!\")\n",
    "    #         break\n",
    "\n",
    "# Load the best model\n",
    "# model.load_state_dict(torch.load(best_model_path))\n",
    "print(\"Best model loaded.\")\n",
    "# plot_loss(train_losses, val_losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
